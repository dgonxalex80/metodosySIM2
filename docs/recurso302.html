<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Métodos y Simulación Estadística" />


<title> Estimación puntual</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"> </a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Probabilidad
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso101.html">Probabilidad</a>
    </li>
    <li>
      <a href="recurso102.html">Conceptos básicos</a>
    </li>
    <li>
      <a href="recurso103.html">Enfoque</a>
    </li>
    <li>
      <a href="recurso104.html">Tipos de probabilidad</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Variable
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso201.html">Variable aleatoria</a>
    </li>
    <li>
      <a href="recurso202.html">Valos esperado y varianza</a>
    </li>
    <li>
      <a href="recurso203.html">Variables conjuntas</a>
    </li>
    <li>
      <a href="recurso204.html">Modelos discretos</a>
    </li>
    <li>
      <a href="recurso205.html">Modelos continuos</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Inferencia
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso301.html">Conceptos básicos</a>
    </li>
    <li>
      <a href="recurso302.html">Estimación puntual</a>
    </li>
    <li>
      <a href="recurso303.html">Propiedades</a>
    </li>
    <li>
      <a href="recurso304.html">Métodos de estimación</a>
    </li>
    <li>
      <a href="recurso305.html">Teorema del Límite Central</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Intervalos
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso401.html">Intervalos una población</a>
    </li>
    <li>
      <a href="recurso402.html">Intervalos dos poblaciones</a>
    </li>
    <li>
      <a href="recurso403.html">Estimación no paramétrica</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Hipótesis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso501.html">Pruebas de hipótesis</a>
    </li>
    <li>
      <a href="recurso502.html">Conceptos básicos</a>
    </li>
    <li>
      <a href="recurso503.html">Pruebas paramétricas</a>
    </li>
    <li>
      <a href="recurso504.html">Pruebas no paramétricas</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Software R
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="recurso601.html">Probabilidad</a>
    </li>
    <li>
      <a href="recurso602.html">Variable aleatória</a>
    </li>
    <li>
      <a href="recurso607.html">Modelos de probabilidad</a>
    </li>
    <li>
      <a href="recurso603.html">Estimación</a>
    </li>
    <li>
      <a href="recurso604.html">Intervalos de confianza</a>
    </li>
    <li>
      <a href="recurso605.html">Pruebas paramétricas</a>
    </li>
    <li>
      <a href="recurso606.html">Pruebas no paramétricas</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore"><span style="color:#034a94">
<strong>Estimación puntual</strong></span></h1>
<h4 class="author">Métodos y Simulación Estadística</h4>

</div>


<p><br/><br/></p>
<p>La estimación se realiza cuando no conocemos el valor de un parámetro
poblacional y lo podemos realizar de manera puntual (un solo valor) o
mediante un intervalo de confianza.</p>
<div id="métodos-de-estimación" class="section level1">
<h1><span style="color:#034a94"><strong>Métodos de
estimación</strong></span></h1>
<p><br/><br/></p>
<p>El método de momentos fue propuesto por Karl Pearson al rededor de
1895, pensado en sus inicios en contexto descriptivo, analizando las
distribuciones de probabilidad y aprisionándolas al sistema de curvas
que llevan su nombre. Posteriormente este concepto fue modificado por
R.A. Fisher en 1920. El método consiste en estimar un parámetro de una
distribución igualando sus momentos teóricos o poblacionales, si
existen, con los correspondientes momentos muestrales.</p>
<p>Para mostrar este método es necesario definir el concepto de
momento.</p>
<p><br/></p>
<div id="momento-poblacional" class="section level2">
<h2><span style="color:#034a94"><strong>Momento
Poblacional</strong></span></h2>
<table>
<colgroup>
<col width="51%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Caso discreto</th>
<th align="center">Caso continuo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span
class="math display">\[\mu^{k}=E\big[X^{k}\big]=\sum_{R_X}
x^{k}p(x)\]</span></td>
<td align="center"><span
class="math display">\[\mu^{k}=E\big[X^{k}\big]=\int_{-\infty}^{\infty}x^{k}f(x)
dx \]</span></td>
</tr>
</tbody>
</table>
<div id="momentos-muestrales" class="section level3">
<h3><span style="color:#034a94"><strong>Momentos
muestrales</strong></span></h3>
<p>En ambos casos</p>
<p><span class="math display">\[m^{k}=\frac{1}{n}\sum_{i=1}^{n}
x_{i}^{k} \]</span></p>
<p><br/></p>
<p>El método de momentos supone que los momentos tanto poblacionales
como muestrales son conocidos, y por lo tanto también la función de
probabilidad.</p>
<p>A continuación se relacionan algunos de estos momentos
poblacionales:</p>
<br/>
<center>
<strong>Tabla 2.7</strong> Valor esperado y varianza de algunos modelos
de probabilidad
</center>
<table>
<colgroup>
<col width="20%" />
<col width="31%" />
<col width="47%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Distribución</th>
<th align="left"><span class="math inline">\(E[X]\)</span></th>
<th align="left"><span
class="math inline">\(V[X]=E[X^{2}]-E[X]^{2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bernoulli</td>
<td align="left"><span class="math inline">\(p\)</span></td>
<td align="left"><span class="math inline">\(pq\)</span></td>
</tr>
<tr class="even">
<td align="left">geométrica</td>
<td align="left"><span
class="math inline">\(\displaystyle\frac{1}{p}\)</span></td>
<td align="left"><span
class="math inline">\(\displaystyle\frac{q}{p^{2}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">binomial</td>
<td align="left"><span class="math inline">\(np\)</span></td>
<td align="left"><span class="math inline">\(npq\)</span></td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left"><span class="math inline">\(\lambda\)</span></td>
<td align="left"><span class="math inline">\(\lambda\)</span></td>
</tr>
<tr class="odd">
<td align="left">gamma</td>
<td align="left"><span class="math inline">\(\alpha\beta\)</span></td>
<td align="left"><span
class="math inline">\(\alpha\beta^{2}\)</span></td>
</tr>
<tr class="even">
<td align="left">exponencial</td>
<td align="left"><span class="math inline">\(\beta\)</span></td>
<td align="left"><span class="math inline">\(\beta^{2}\)</span></td>
</tr>
<tr class="odd">
<td align="left">uniforme</td>
<td align="left"><span
class="math inline">\(\displaystyle\frac{a+b}{2}\)</span></td>
<td align="left"><span
class="math inline">\(\displaystyle\frac{(b-a)^{2}}{12}\)</span></td>
</tr>
<tr class="even">
<td align="left">normal</td>
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="left"><span class="math inline">\(\sigma^{2}\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Nota: Existe una relación entre las distribuciones Poisson y
exponencial.Se podrían dar en función de <span
class="math inline">\(\lambda\)</span>, haciendo <span
class="math inline">\(\beta=\dfrac{1}{\lambda}\)</span></p>
<p><br/><br/></p>
</div>
<div id="ejemplo" class="section level3">
<h3><span style="color:#FF7F00"> <strong>Ejemplo</strong></span></h3>
<p><br/></p>
<p>Encuentre los estimadores de los parámetros de la distribución normal
a través del método de momentos. Previamente sabemos que los parámetros
de una variable con distribución normal son <span
class="math inline">\(E[X]=\mu\)</span> y <span
class="math inline">\(V[X]=\sigma^{2}\)</span> y que <span
class="math inline">\(V[X]=E[X^{2}]-E[X]^{2}\)</span>. Dada esta
información el estimador de momentos se construye de la siguiente
manera:</p>
<p><span class="math display">\[M^{1}=m^{1}\]</span></p>
<p><span class="math display">\[M^{2}=m^{2}\]</span></p>
<p>Aplicando el método:</p>
<p><br/></p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><span class="math display">\[M^{1}= m^{1}\]</span></td>
</tr>
<tr class="even">
<td align="left"><span
class="math display">\[\mu  =  \displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}\]</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math display">\[\widehat{\mu} =
\displaystyle\frac{1}{n}\sum_{i=1}^{n} x_{i}=\bar{x}\]</span></td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Para estimar <span class="math inline">\(\sigma^{2}\)</span>, se
realiza el siguiente procedimiento, usando <span
class="math inline">\(\mu^{1}=m^{1}\)</span> y <span
class="math inline">\(\mu^{2}=m^{2}\)</span>.</p>
<p><span class="math display">\[V[X]=E[X^{2}]-E[X]^{2} =
\mu^{2}-(\mu^{1})^{2}\]</span></p>
<p>entonces igualamos estos dos momentos poblacionales con sus
respectivos momentos muestrales quedando la igualdad</p>
<p><span class="math display">\[\begin{eqnarray*}
    V[X]&amp;=&amp; \mu^{2}-(\mu^{1})^{2}\\
    &amp;=&amp;m^{2}-(m^{1})^{2}\\
    &amp;=&amp;\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\bar{x}^{2}
\end{eqnarray*}\]</span></p>
<p>podemos representar la varianza por <span
class="math inline">\(\sigma^{2}\)</span> y obtenemos</p>
<p><span
class="math display">\[\widehat{\sigma^{2}}=\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\bar{x}^{2}\]</span></p>
<p>y obtenemos el estimador de la varianza:</p>
<p><br/></p>
<table>
<colgroup>
<col width="18%" />
<col width="81%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="right"><span
class="math inline">\(\widehat{\sigma^{2}}\)</span></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\bar{x}^{2}\)</span></td>
</tr>
<tr class="even">
<td align="right"><span
class="math inline">\(\widehat{\sigma^{2}}\)</span></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\bar{x}^{2}-\bar{x}^{2}+\bar{x}^{2}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-2\bar{x}^{2}+\bar{x}^{2}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\displaystyle\frac{2\bar{x}\sum
x_{i}}{n}+\displaystyle\frac{n \bar{x}^{2}}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\Big(\sum_{i=1}^{n}
x_{i}^{2}-2\bar{x}\sum_{i=1}^{n} x_{i}+\bar{x}^{2}\Big)\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"><span class="math inline">\(=
\displaystyle\frac{1}{n}\sum_{i=1}^{n}\Big(x_i-\bar{x}\Big)^{2}\)</span></td>
</tr>
</tbody>
</table>
<p><br/><br/></p>
<p>En resumen los estimadores de momentos para los parámetros de la
distribución normal son:</p>
<p><span class="math display">\[\widehat{\mu} =
\displaystyle\frac{1}{n}\sum_{i=1}^{n} x_{i}=\bar{x}\]</span></p>
<p><span class="math display">\[\widehat{\sigma^{2}} =
\displaystyle\frac{1}{n}\sum_{i=1}^{n}\Big(x-\bar{x}\Big)^{2}\]</span></p>
<p>A partir de ellos y mediante la obtención de una muestra aleatoria
por ejemplo :630, 650, 710, 750, 790, 820, 860 y 910 se pueden estimar
los parámetros por método de momentos con los siguientes resultados:</p>
<p><span class="math display">\[\widehat{\mu}=765\]</span></p>
<p><span class="math display">\[\widehat{\sigma^{2}}=8550\]</span></p>
</div>
</div>
<div id="método-de-máxima-verosimilitud" class="section level2">
<h2><span style="color:#034a94"><strong>Método de Máxima
Verosimilitud</strong></span></h2>
<p>Uno de los mejores métodos para obtener un estimador puntual de un
parámetro es el método de <strong>Máxima Verosimilitud</strong> o de
máxima probabilidad. Esta técnica fue desarrollada en 1920 por el
estadístico britanico Sir R.A. Fisher, afirmando que el estimador será
el valor del parámetro que maximice la función de verosimilitud <span
class="math inline">\(L(\theta)\)</span>. \ \</p>
<p>La función de verosimilitud <span
class="math inline">\(L(\theta)\)</span> corresponde a la función de
distribución conjunta de variables aleatorias independientes con igual
función de distribución. Estas variables aleatorias corresponden a las
variables que conforman la muestra. % <span
class="math display">\[L(\theta)=f(x_{1},\theta).f(x_{2},\theta).f(x_{3},\theta)....f(x_{n}),\theta)\]</span>
El objetivo del método será encontrar el valor del parámetro que
maximice la probabilidad conjunta, suponiendo el conocimiento de la
función de distribución de probabilidad de la variable en estudio.</p>
<p></br></br></p>
<div id="ejemplo-1" class="section level3">
<h3><span style="color:#FF7F00"> <strong>Ejemplo</strong></span></h3>
<p>Para el caso de la distribución normal cuya función de distribución
de probabilidad esta dada por :</p>
<p><span class="math display">\[f(x_{i})=\frac{1}{\sqrt{2\pi}\sigma^{2}}
\exp{\Bigg(-\frac{1}{2\sigma^{2}}\big(x_{i}-\mu\big)^{2}\Bigg)}\]</span>
La función de verosimulitud estará dada por:</p>
<p><span
class="math display">\[L(x_{1},x_{2},..,x_{n};\mu,\sigma^{2})=f(x_{1};\mu,\sigma^{2})....f(x_{n};\mu,\sigma^{2})\]</span></p>
<p>Esta función se puede escribir como :</p>
<p><span
class="math display">\[L(x_{1},\cdots,x_{n};\mu,\sigma^{2})=\displaystyle\prod_{i=1}^{n}
\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp{\Bigg(-\frac{1}{2\sigma^{2}}\big(x_{i}-\mu\big)^{2}\Bigg)}
\]</span></p>
<p><span class="math display">\[L=\displaystyle\Big(\frac{1}{2\pi
\sigma^{2}}\Big)^{n/2} \exp
\Bigg(\sum_{i=1}^{n}\frac{-1}{2\sigma^{2}}(x_{i}-\mu)^{2}\Bigg)
\]</span></p>
<p><span class="math display">\[L=\displaystyle\Big(2\pi
\sigma^{2}\Big)^{-n/2} \exp
\Bigg(\frac{-1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\Bigg)
\]</span></p>
<p>El método consiste en encontrar el valor del parámetro que maximice
esta función para lo cual procedemos a derivar <span
class="math inline">\(L\)</span> parcialmente con respecto al parámetro
objetivo, por ejemplo <span class="math inline">\(\mu\)</span>.</p>
<p>Este proceso presenta algunas dificultades de cálculo que son
atenuadas mediante la premisa de que el <em>máximo de la función <span
class="math inline">\(L\)</span> corresponde a los mismos máximos de la
función <span class="math inline">\(\ln(L)\)</span></em>, la cual es más
sencilla de derivar. Este procedimiento es posible debido a que la
función <span class="math inline">\(L\)</span> es creciente.</p>
<p>Convertimos <span class="math inline">\(L\)</span> en <span
class="math inline">\(ln(L)\)</span></p>
<p><span class="math display">\[\ln(L)= -\displaystyle\frac{n}{2}
\ln(2\pi) - \displaystyle\frac{n}{2} \ln(\sigma^{2})
-\displaystyle\frac{1}{2\sigma^{2}}\displaystyle\sum_{i=1}^{n}(x_{i}-\mu)^{2}\]</span>
Al derivar parcialmente <span class="math inline">\(\ln(L)\)</span> con
respecto a <span class="math inline">\(\mu\)</span> tenemos:</p>
<p><span class="math display">\[\displaystyle\frac{\partial
\ln(L)}{\partial \mu}=
\displaystyle\frac{2}{2\sigma^{2}}  \displaystyle\sum_{i=1}^{n}
(x_{i}-\mu) =0\]</span></p>
<p>De esta igualdad se despeja el parámetro de interés</p>
<p><span class="math display">\[\sigma^{2} \hspace{.2cm}
\frac{1}{\sigma^{2}}\sum_{i=1}^{n} (x_{i}-\mu) =0
\hspace{.2cm}  \sigma^{2} \]</span> <span
class="math display">\[\sum_{i=1}^{n} x_{i} - n \mu =0\]</span></p>
<p><span class="math display">\[\widehat{\mu}=\frac{1}{n}\sum_{i=1}^{n}
x_{i} = \bar{x}\]</span></p>
<p>En el caso de la estimación de <span
class="math inline">\(\sigma^{2}\)</span>, se deriva <span
class="math inline">\(\ln(L)\)</span> parcialmente con respecto a <span
class="math inline">\(\sigma^{2}\)</span>, se iguala a cero el resultado
obtenido y por último se despeja <span
class="math inline">\(\sigma^{2}\)</span>. Verifique que el estimador de
máxima verosimilitud para la varianza es igual a:</p>
<p><span
class="math display">\[\widehat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}
\big(x_{i}-\mu \big)^{2} \]</span> <br/><br/><br/></p>
</div>
</div>
<div id="distribuciones-muestrales" class="section level2">
<h2><span style="color:#034a94"><strong>Distribuciones
muestrales</strong></span></h2>
<p><br/></p>
<div id="principales-distribuciones-muestrales" class="section level3">
<h3><span style="color:#034a94"><strong>Principales distribuciones
muestrales</strong></span></h3>
<ul>
<li>normal</li>
<li>Chi-cuadrado,</li>
<li>t-student,</li>
<li>F-Fisher</li>
</ul>
<p>Se denomina distribución muestral a la distribución que sigue los
principales estimadores puntuales o funciones de ellos como son:</p>
<p><span class="math display">\[\bar{X},\hspace{.2cm} \widehat{p},
\hspace{.2cm}\frac{(n-1)S^ {2}}{\sigma^{2}}, \hspace{.2cm}
Z=\frac{\bar{X}-\mu}{S/\sqrt{n}}, \hspace{.2cm} F=\frac{\chi^
{2}_{1}/v_{1}}{\chi^{2}_{2}/v_{2}}\]</span></p>
<p>Existe una relación importante entre ellas con la Distribución
normal, las cuales se representan en el siguiente diagrama:</p>
<p><br/></p>
<center>
<p><img src="img/Distribuciones.png" width="70%" style="display: block; margin: auto;" /></p>
<p><strong>Figura 2.32</strong> Relación de los modelos de probabilidad
muestrales </br></br></p>
<p>Si una variable <strong>X</strong> se distribuye
<strong>normal</strong> con parámetros <span
class="math inline">\(\mu\)</span> y <span
class="math inline">\(\sigma^{2}\)</span>, entonces a partir de su
transformación se puede obtener una variable <strong>Z</strong> que
tenga distribución <strong>normal estándar</strong> <span
class="math inline">\(N(0,1)\)</span>, que tiene media cero y varianza
uno. Varias normales estándar al cuadrado conforman una variable con
distribución <strong>chi-cuadrado</strong> y a su vez esta variable
tiene la propiedad tal que la suma de <span
class="math inline">\(n\)</span> variables aleatorias <span
class="math inline">\(\chi^{2}\)</span> con <span
class="math inline">\(n-1\)</span> grados de libertad. La razón de una
variable con distribución normal estándar y la raíz cuadrada de una
variable chi-cuadrado, dividida por sus grados de libertad originan una
variable con distribución <strong>t-student</strong>. Por último la
razón de dos variables con distribución chi-cuadrado, divididas cada una
por sus respectivos grados de libertad, generan una variable con
distribución <strong>F-Fisher</strong>.</p>
<p>En sintesis, la distribuciones de muestreo básicas, tienen de base la
distribución normal.</p>
<p><br/><br/></p>
</div>
</div>
<div id="distribución-chi-cuadrado" class="section level2">
<h2><span style="color:#034a94"><strong>Distribución
chi-cuadrado</strong></span></h2>
<p><br/></p>
<p>Si <span class="math inline">\(S^{2}\)</span> es la varianza de la
muestra aleatoria de tamaño <span class="math inline">\(n\)</span> que
se toma de una población normal que tiene varianza <span
class="math inline">\(\sigma^{2}\)</span>, entonces el estadístico:
<span
class="math display">\[\chi^{2}=\frac{(n-1)S^{2}}{\sigma^{2}}=\sum_{i=1}^{n}\frac{\big(x_{i}-\bar{x}\big)^{2}}{\sigma^{2}}=\sum_{i=1}^{n}
\Big(\frac{x_{i}-\bar{x}}{\sigma}\Big)^{2} \]</span> tiene una
distribución chi-cuadrado con <span class="math inline">\(v=n-1\)</span>
grados de libertad.</p>
<p>Nota: Esta función fue creada por Karl Pearson científico inglés
(1857-1936) y su función de distribución y su representación gráfica
están dadas por:</p>
<p><span
class="math display">\[f(x)=\frac{1}{2^{(v/2)}\Gamma(v/2)}x^{(v/2)-1}\exp\{-x/2\}
,\hspace{.3cm} x&gt;0\]</span> <br/><br/></p>
<center>
<img src="recurso302_files/figure-html/unnamed-chunk-2-1.png" width="672" />
<br/> <strong>Figura 2.33</strong> Función de densidad chi-cuadrado
</center>
<p><br/></p>
<p>Esta distribución es un caso especial de la distribución Gamma con
parámetros <span class="math inline">\(\alpha=\frac{v}{2}\)</span> y
<span class="math inline">\(\beta=2\)</span></p>
<p><br/><br/></p>
</div>
<div id="distrución-t-student" class="section level2">
<h2><span style="color:#034a94"><strong>Distrución
t-student</strong></span></h2>
<p><br/></p>
<p>Esta función nace de la relación entre una variable con distribución
normal estándar y la raíz cuadrada de una variable con distribución
chi-cuadrado Sea Z una variable con distribución normal estándar y V una
variable con distribución chi-cudadrado con v grados de libertad,
entonces la variable aleatoria T se distribuye t-student con v grados de
libertad</p>
<p><span class="math display">\[T=\frac{Z}{\sqrt{V/v}} \sim
t_{v}\]</span></p>
<p>Nota: Esta función de distribución fue propuesta por William Sealy
Gosset en 1908. Gosset trabajaba en una fábrica de cerveza de prppiedad
de Guiness, quien prohibía a sus empleados la publicación de artículos
cientificos debido a la difución previa de secretos industriales. Debido
a esta prohibición Gosset publicaba sus artículos con el seudónimo de
Student.(Wikipedia.org)</p>
<p>La función de probabilidad de distribución t-student y su
representación gráfica están dadas por:</p>
<p><span
class="math display">\[f(x)=\frac{\Gamma[(v+1)/2]}{\Gamma[v/2]\sqrt{\pi
v}}\Bigg(1+\frac{x^{2}}{v} \Bigg)^{-(v+1)/2}, \hspace{.3cm} -\infty&lt;
x&lt;\infty \]</span></p>
<center>
<img src="recurso302_files/figure-html/unnamed-chunk-3-1.png" width="672" />
<br/> <strong>Figura 2.34</strong> Función de densidad t-Student
</center>
<p><br/></p>
<p>La gráfica de la distribución t-student es similar a la de la
distribución normal, salvo que sus colas son más pesadas.</p>
<p>Si <span class="math inline">\(X_{1},X_{2},...,X_{n}\)</span> es una
muestra aleatoria de <span class="math inline">\(n\)</span> variables
independientemente e idénticamente distribuidas <span
class="math inline">\(N(\mu,\sigma^{2})\)</span> y</p>
<p><span class="math display">\[\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}
\]</span></p>
<p><span
class="math display">\[S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\Big(X_{i}-\bar{X}\Big)^{2}
\]</span></p>
<p>Entonces la variable aleatoria:</p>
<p><span class="math display">\[T=\frac{\bar{X}-\mu}{S/\sqrt{n}}
\]</span></p>
<p>tiene una distribución t-student con <span
class="math inline">\(v=n-1\)</span> grados de libertad</p>
<p><br/><br/></p>
</div>
<div id="distribución-f-fisher" class="section level2">
<h2><span style="color:#034a94"><strong>Distribución
F-Fisher</strong></span></h2>
<p><br/></p>
<p>Si <span class="math inline">\(S_{1}^{2}\)</span> y <span
class="math inline">\(S_{2}^{2}\)</span> son las dos varianzas de
muestras aleatorias independientes de tamaños <span
class="math inline">\(n_{1}\)</span> y <span
class="math inline">\(n_{2}\)</span> tomadas de poblaciones normales con
varianzas <span class="math inline">\(\sigma_{1}^{2}\)</span> y <span
class="math inline">\(\sigma_{2}^{2}\)</span> respectivamente,
entonces:</p>
<p><span
class="math display">\[F=\frac{S_{1}^{2}/\sigma_{1}^{2}}{S_{2}^{2}/\sigma_{1}^{2}}=\frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}
\frac{S_{1}^{2}}{\sigma_{2}^{2}} \]</span></p>
<p>tiene una distribución F con grados de libertad <span
class="math inline">\(v_{1}=n_{1}-1\)</span> y <span
class="math inline">\(v_{2}=n_{2}-1\)</span>\</p>
<div id="nota" class="section level3">
<h3><strong>NOTA</strong>:</h3>
<p>Ronald Aylmer Fisher (1890-1962) científico, matemático, estadístico,
biólogo evolutivo y genetista inglés fue el creador de la distribución
F. Su función de distribución y su representación gráfica están dadas
por:</p>
<p><span class="math display">\[f(x)=\displaystyle\frac{\Gamma
\Bigg[\displaystyle\frac{v_{1}+v_{2}}{2}\Bigg]\Bigg(\displaystyle\frac{v_{1}}{v_{2}}\Bigg)^{v_{1}/2}
x^{(v_{1}/2)-1}}{\Gamma \Bigg[\displaystyle\frac{v_{1}}{2}\Bigg] \Gamma
\Bigg[\displaystyle\frac{v_{2}}{2}\Bigg]
    \Bigg[\Bigg(\displaystyle\frac{v_{1}}{v_{2}}\Bigg)x+1\Bigg]^{\big(v_{1}+v_{2}\big)/2}},
\hspace{.3cm} x&gt;0 \]</span></p>
<center>
<p><img src="recurso302_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<br/> <strong>Figura 2.35</strong> Función de densidad F de Fisher
</center>
<p><br/></p>
<p><br/><br/></p>
</div>
</div>
<div id="media-muestral" class="section level2">
<h2><span style="color:#034a94"><strong>Media
muestral</strong></span></h2>
<p><br/></p>
<p><span class="math display">\[ \bar{X}=\frac{1}{n}\sum_{i=1}^{n}
X_{i}\]</span></p>
<p>Al hablar de distribución de probabilidad de la media muestral,
implícitamente estamos afirmando que <span
class="math inline">\(\bar{X}\)</span>, es una variable aleatoria. De
esta variable a continuación estudiaremos sus principales
características.</p>
<p>Para verificar las propiedades de este estimador podemos hacer uso de
las propiedades del valor esperado de la siguiente manera:</p>
<p><span class="math display">\[\begin{eqnarray*}
    E\big[\bar{X}\big] &amp;=&amp; E\Bigg[
\frac{1}{n}\Big[X_{1}+X_{2}+\cdots + X_{n}\Big]\Bigg]\\  
    &amp;=&amp; \frac{1}{n} E\Big[X_{1}+X_{2}+\cdots + X_{n}\Big]\\
    &amp;&amp; \frac{1}{n}
\Bigg[E\big[X_{1}\big]+E\big[X_{2}\big]+\cdots +
E\big[X_{n}\big]\Bigg]\\
    &amp;=&amp;\frac{1}{n} \big[\mu + \mu \cdots + \mu \big] \\
    &amp;=&amp; \frac{1}{n} n\mu \\
    &amp;=&amp; \mu
\end{eqnarray*}\]</span></p>
<p>por tanto:</p>
<p><span class="math display">\[\mu = \mu_{\bar{X}}\]</span></p>
<p>Como conclusión obtenemos que la media de la media muestral es igual
a la media de la variable.</p>
<p>Con la varianza ocurre algo similar al utilizar las propiedades de la
varianza:</p>
<p><span class="math display">\[\begin{eqnarray*}
    V[\bar{X}]&amp;=&amp;V\Bigg[\frac{1}{n} \sum_{i=1}^{n} x_{i}\Bigg]
\\
    &amp;=&amp;\dfrac{1}{n^{2}} V\big[X_{1}+X_{2}+\cdots + X_{n}\big] \\
    &amp;=&amp; \dfrac{1}{n^{2}} V[X_{1}]+V[X_{2}]+\cdots +V[X_{n}]\\
    &amp;=&amp; \dfrac{1}{n^{2}} n\sigma^{2}\\
    &amp;=&amp; \dfrac{\sigma^{2}}{n}
\end{eqnarray*}\]</span></p>
<p>Se concluye que:</p>
<span class="math display">\[V[\bar{X}] = \dfrac{\sigma^{2}}{n}\]</span>
<div>
<p>Obtenemos así que la varianza de la media muestral es igual a la
varianza de la variable.</p>
<p>De estos resultados podemos concluir que:</p>
<ul>
<li>La media poblacional de la variable aleatoria <span
class="math inline">\(X\)</span> es idéntica a la media de la media
poblacional de la media <span
class="math inline">\(\bar{X}\)</span></li>
</ul>
<p><span class="math display">\[\mu_{_{X}} =
\mu_{_{\bar{X}}}\]</span></p>
<ul>
<li>La varianza poblacional de <span
class="math inline">\(\bar{X}\)</span> es igual la varianza de <span
class="math inline">\(X\)</span> dividido por el tamaño de la
muestra</li>
</ul>
<p><span class="math display">\[\sigma^{2}_{_{\bar{X}}} =
\dfrac{\sigma^{2}_{_{X}}}{n}\]</span></p>
</div>
<p><br/><br/></p>
<p><span class="math display">\[\text{factor por corrección por
población finita : fcpf = } \dfrac{(N-n)}{(N-1)}\]</span></p>
<p><br/><br/></p>
<div id="ejemplo-2" class="section level3">
<h3><span style="color:#FF7F00"> <strong>Ejemplo</strong></span></h3>
<p><br/></p>
<p>Cierto tipo de batería para automóviles dura un promedio de 1110 días
con una desviación estándar de 80 días. Si se eligen n=400 de estas
baterías de manera aleatoria, encuentre la probabilidad de que el
<strong>promedio del tiempo de duración</strong> sea mayor a 1120 días.
Como <span class="math inline">\(X \sim N(\mu=1110,
\sigma^{2}=80^{2})\)</span>, utilizando las propiedades verificadas
anteriormente podemos afirmar que: <span class="math inline">\(\bar{X}
\sim N(\mu=1110, \sigma_{\bar{X}}=80/\sqrt{400})\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
    P(\bar{x} &gt; 1120) &amp;=&amp;
P\Bigg(\dfrac{\bar{x}-\mu}{\sigma/\sqrt{n}} &gt;
\dfrac{1120-1110}{80/\sqrt{400}}\Bigg)\\
    &amp;=&amp; P(Z &gt; 2.5)\\
    &amp;=&amp; 1-P(Z &lt; 2.5)\\
    &amp;=&amp; 1-0.9938 \\
    &amp;=&amp; 0.0068
\end{eqnarray*}\]</span></p>
<p><br/><br/><br/><br/></p>
</div>
</div>
<div id="proporción-muestral" class="section level2">
<h2><span style="color:#034a94"><strong>Proporción
muestral</strong></span></h2>
<p><br/></p>
<p>Al igual que en la media podemos realizar la verificación las
características de la proporción</p>
<p><span
class="math display">\[E[\hspace{.1cm}\widehat{p}\hspace{.1cm}]=
p\]</span></p>
<p><span class="math display">\[V[\hspace{.1cm}\widehat{p}\hspace{.1cm}]
=\dfrac{p(1-p)}{n}\]</span></p>
<p><br/><br/></p>
<div id="ejemplo-3" class="section level3">
<h3><span style="color:#FF7F00"> <strong>Ejemplo</strong></span></h3>
<p><br/></p>
<p>La proporción de individuos con tipo de sangre Rh positivo es de 85%.
Si se tiene una muestra de <span class="math inline">\(n=500\)</span>
individuos, Encuentre la probabilidad de que la proporción muestral
<span class="math inline">\(\widehat{p}\)</span> pase de 82%. Cuando el
tamaño de muestra es grande, la distribución de la proporción se
aproxima a una distribución normal con media <span
class="math inline">\(p\)</span> y varianza <span
class="math inline">\(p(1-p)/n\)</span>. De esta manera podemos hallar
la probabilidad solicitada:</p>
<p><span class="math display">\[\begin{eqnarray*}
    P(\widehat{p}&gt;0.82) &amp;=&amp;
P\Bigg(\dfrac{\widehat{p}-p}{(p(1-p)/\sqrt{n}} &gt; \dfrac{0.82 -
0.84}{\sqrt{(0.85 \times 0.15)/500}}\Bigg) \\
    &amp;=&amp; P\big(Z &gt; -1.878 \big) \\
    &amp;=&amp; 1 - P(Z &lt; -1.88)\\
    &amp;=&amp; 1-0.0301
\end{eqnarray*}\]</span></p>
<p><br/><br/></p>
</div>
</div>
<div id="varianza-muestral" class="section level2">
<h2><span style="color:#034a94"><strong>Varianza
muestral</strong></span></h2>
<p><br/></p>
<p>Ahora tenemos interés en estudiar el comportamiento de <span
class="math inline">\(S^{2}\)</span> llamada cuasivarianza por algunos y
definida por:</p>
<p><span
class="math display">\[\widehat{\sigma^{2}}=S^{2}=\dfrac{1}{(n-1)}\sum_{i=1}^{n}\big(x_{i}-\bar{x}\big)^{2}\]</span></p>
<p>Se puede demostrar que:</p>
<p><span class="math display">\[E\big[S^{2}\big] =
\sigma^{2}\]</span></p>
<p><span class="math display">\[V[S^{2}] =
\dfrac{2\sigma^{4}}{(n-1)}\]</span></p>
<p>Pero también que:</p>
<p><span class="math display">\[\dfrac{(n-1) S^{2}}{\sigma^{2}} \sim
\chi^{2}_{(v=n-1)} \]</span></p>
<p>Revisar : Canavos(1988) pp.231</p>
<p><br/><br/><br/></p>
</div>
<div id="algunos-estimadores" class="section level2 content-box-blue">
<h2><span style="color:#034a94"><strong>Algunos
estimadores</strong></span></h2>
<br/>
<center>
<p><strong>Tabla 2.7</strong> Principales características de algunos
estimadores</p>
<table>
<colgroup>
<col width="13%" />
<col width="25%" />
<col width="21%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parámetro</th>
<th align="center">Estimador</th>
<th align="center">Valor esperado</th>
<th align="center">Varianza</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\theta\)</span></td>
<td align="center"><span
class="math inline">\(\widehat{\theta}\)</span></td>
<td align="center"><span
class="math inline">\(E[\widehat{\theta}]\)</span></td>
<td align="center"><span
class="math inline">\(V[\widehat{\theta}]\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span class="math inline">\(\bar{x}\)</span></td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span
class="math inline">\(\displaystyle\frac{\sigma^{2}}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span
class="math inline">\(\widehat{p}=\frac{X}{n}\)</span></td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span
class="math inline">\(\displaystyle\frac{pq}{n}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\sigma^{2}\)</span></td>
<td align="center"><span
class="math inline">\(S^{2}=\displaystyle\frac{\displaystyle\sum_{i=1}^{n}(x-\bar{x})^{2}}{n-1}\)</span></td>
<td align="center"><span class="math inline">\(\sigma^{2}\)</span></td>
<td align="center"><span
class="math inline">\(\dfrac{2\sigma^{4}}{(n-1)}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span
class="math inline">\(\mu_{1}-\mu_{2}\)</span></td>
<td align="center"><span
class="math inline">\(\bar{x}_{1}-\bar{x}_{2}\)</span></td>
<td align="center"><span
class="math inline">\(\mu_{1}-\mu_{2}\)</span></td>
<td align="center"><span
class="math inline">\(\dfrac{\sigma^{2}_{1}}{n_{1}}+\dfrac{\sigma^{2}_{2}}{n_{2}}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(p_{1}-p_{2}\)</span></td>
<td align="center"><span
class="math inline">\(\widehat{p_{1}}-\widehat{p_{2}}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}-p_{2}\)</span></td>
<td align="center"><span
class="math inline">\(\dfrac{p_{1}q_{1}}{n_{1}}+\dfrac{p_{2}q_{2}}{n_{2}}\)</span></td>
</tr>
</tbody>
</table>
</center>
</div>
<p><br/><br/><br/></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
