---
title : <span style="color:#034a94"> **Pruebas no paramétricas**</span>
author: "Métodos y Simulación Estadística"
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, comment = NA)

# install.packages('gtools')
# install.packages("TeachingSampling")

#load library
library(gtools)
library(TeachingSampling)
library(readr)
library(paqueteMET)

c1="#FF7F00"
c2="#034A94"
c3="#0EB0C6"
c4="#686868"

data("CarreraLuz22")
data("evaluacion")

```

<br/><br/><br/>

Hasta el momento nos hemos dedicado a la estadística paramétrica, que exige la estimación de parámetros y de la comprobación de supuestos sobre las distribuciones de las variables, como por ejemplo que se distribuyan normal.

Note que la distribución normal esta relacionada con las distribuciones t-student, chi-cuadrado y F entre otras.

### **Cuando utilizar pruebas no paramétricas**:

+ Cuando no se cumplen los supuestos como Normalidad
+ Tamaños mínimos de muestra
+ Número igual de elementos  en cada muestra
+ Homogeneidad de varianza, etc

+ Cuando se usan tamaños de muestra pequeños
+ Menores a 30 que no permiten comprobar supuestos sobre la población.

+ Cuando se convierten datos cualitativas (escalas nominales u ordinales ) a información útil para la toma de decisiones (escala de intervalo). Utilizado en estudios mercadeo para medir variables como gustos, satisfacción, nivel de necesidad etc.

### **Ventajas de utilizar pruebas no paramétricas**

+ Son fáciles de usar (aunque dada la facilidad de los códigos en R ambas se corren con una línea)
+ No se requieren comprobar supuestos
+ Se pueden usar con muestras  pequeñas
+ Se pueden usar con variables  cualitativas

### **Desventajas** 

+ Ignoran información
+ No son tan eficientes como las pruebas paramétricas, tienen menor potencia.
+ Llevan a una mayor probabilidad de cometer error tipo II ( no rechazar Ho falsa)


Entre las principales pruebas no paramétricas están :

+ Prueba Chi-Cuadrado de Independencia
+ Prueba Chi-Cuadrado de Bondad de Ajuste
+ Prueba de Signos 
+ Prueba de Rachas
+ Prueba Wilcoxon
+ Prueba de Mann-Whitney
+ Prueba de Kruskal-Wallis
+ Correlación de Rangos de Sperarman

<br/><br/>

### **Prueba de signos**  

|            |                                           |
|:---------- |:----------------------------------------------------------------|
| No. de grupos de datos |		1 o 2                                            |
| Variable dependiente	 | En escala al menos ordinal                          |	
| Objetivo               | Esta prueba puede ser utilizada para determinar si la diferencia entre el numero de veces que los datos caen a un lado de la media verdadera es significativamente diferente al número de veces que cae en el otro lado . Determinar si la diferencia entre el numero de veces en que el valor de una variable es mayor que el de la otra y el numero de veces que es menor es estadísticamente significativa. Versión no paramétrica de la prueba t para una muestra o de la prueba  t para muestras pareadas. Esta prueba se realiza sobre la mediana de los datos $Me$. |


**Ejemplo**

Carlos y Ángela, Administradoras investigadores de una firma de artículos deportivos tienen la creencia que el deporte afecta la imagen que cada persona tiene de si misma. Para investigar esta posibilidad eligieron a 18 personas de manera aleatoria, para participar en un programa de ejercicios. Antes de empezar el programa las personas respondieron un cuestionario para medir su propia imagen.  Un nivel de 15 puntos en la prueba establece que la persona tiene un concepto indiferente frente a la afirmación, valores menores de 15 que la afecta en forma negativa y valores por encima de 15 que afectan su imagen en forma positiva. Los siguientes son resultados obtenidos : 16, 15, 12, 17, 18, 14, 16, 14, 16,  17, 19, 16,  14, 21, 20, 16, 16, 16  |


```{r}
x.img=c(16, 15, 12, 17, 18, 14, 16, 14, 16,  17, 19, 16,  14, 21, 20, 16, 16, 16 )
```

**Solución**:

Hipótesis Nula	$Ho: Me = 15$ 

Hipótesis Alterna  $Ha : Me > 15$ 

Estadístico de Prueba : $M +$ : Numero de signos positivos

|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|12  |14  |14  |14  |15  |16  |16  |16  |16  |16  |16  |16  |17  | 17 | 18 |19  |20  |21  |
|-   |-   |-   |-   |    |+   |+   |+   |+   |+   |+   |+   |+   |+   |+   |+   |+   |+   |

+ número de signos positivos : 13
+ número de signos negativos :  4
+ en el valor 15 se omite el signo

M+ = 13
Regla de Decisión  (Distribución Binomial  p=0.5 n= 17)
$P(X\leq 13) =  1-0.9755 =  0.0245$  ( valor-p)

**Conclusión**:

Se rechaza la hipótesis nula, se acepta la hipótesis alterna. Se puede   afirmar que las personas tienen la creencia que el deporte afecta positivamente su imagen.

```{r}
library(BSDA)
SIGN.test(x.img,md=15,alternative = "greater")
```

<br/><br/>

### **Prueba chi-cuadrado de bondad de ajuste**

|            |                                                                 |
|:---------- |:----------------------------------------------------------------|
|Tipo de variable|  Cualitativa                                                |
|Número de categorías |$k > 2$ valores                                         |       
|Objetivo	    |Determinar si la diferencia entre las frecuencias de cada uno de los valores de la variable y unas determinadas frecuencias teóricas son estadísticamente significativas.  Utilizada para comprobar el supuesto de normalidad, o de otras distribuciones.                             |

<br/><br/>

**Ejemplo** 

El dueño de una panadería tiene la posibilidad de controlar los niveles de inventarios de leche para cuatro marcas diferentes. Con el fin de establecer políticas para la realización de nuevos pedidos requiere saber si la demanda de estas marcas son iguales (distribución uniforme). Con este propósito tomo la información de un día: |

|Producto            | Marca 1  | Marca 2 | Marca 3   | Marca 4| 
|-------------------:|:--------:|:-------:|:---------:|:------:|
|ventas (obs)        |33        |22       |21         |24      |
|prob.esperada (esp) | 0.25     |0.25     | 0.25      | 0.25   |
|de acuerdo a Ho     |          |         |           |        |

**Solución**:

Ho: Las ventas se distribuyen de manera uniforme (cantidad demandada es igual para todas las marcas)
Ha: Las ventas no tienen una distribución uniforme


```{r}
obs=c(33,22,21,24)
esp=c(0.25,0.25,0.25,0.25)
chisq.test(x=obs,p=esp)
```


<br/><br/>

**Ejemplo**

Un profesor desea establecer si los resultados obtenidos por sus estudiantes tienen una distribución normal con media 3.5 y desviación estándar 0.7 puntos. Los datos se resumen en la siguiente tabla:  

```{r}
nf=c(4.1, 2.7, 3.1, 3.2, 3.0, 3.2, 2.0, 2.4, 1.6, 3.2, 3.1, 2.6, 2.0, 2.4, 2.8, 3.3, 4.0, 3.4, 3.0, 3.1, 2.7, 2.7, 3.0, 3.8, 3.2, 2.2, 3.5, 3.5, 3.8, 3.5, 3.9, 4.2, 4.3, 3.9, 3.2, 3.5, 3.5, 3.7, 4.1, 3.7, 3.5, 3.6, 3.2, 3.1, 3.4, 3.0, 3.0, 3.0, 2.7, 1.7, 3.6, 2.1, 2.4, 3.0, 3.1, 2.5, 2.5, 3.6, 2.2, 2.4, 3.1, 3.3, 2.7, 3.7, 3.0, 2.7, 3.0, 3.2, 3.1, 2.4, 3.0, 2.7, 2.5, 3.0, 3.0, 3.0, 3.2, 3.1, 3.8, 4.1, 3.7, 3.5, 3.0, 3.7, 3.7, 4.1, 3.7, 3.9, 3.7, 2.0)

h=hist(nf, las=1, col=c2, main=" ", xlab=" ", ylab=" ")
lim.nf=h$breaks # cortes de los intervalos
obs.nf=h$counts   # valores observados 
mx=mean(nf)     # media
sdx=sd(nf)          # desviación estandar
Fx.nf=pnorm(lim.nf, mx,sdx)   # probabilidades en caso de que Ho sea verdadera
prob.nf=Fx.nf[2:7]-Fx.nf[1:6]
esp.nf=prob.nf+0.002300453     # ajuste para que el sum(esp.nf)=1

chisq.test(x=obs.nf,p=esp.nf)

```


<br/><br/>

### **Prueba chi-cuadrado de independencia**

<br/><br/>


**Ejemplo**

Un investigador desea establecer la relación que puede existir entre la calificación obtenida por un producto por parte de sus consumidores y su ubicación de residencia. Con este fin recoge información de 100 de sus clientes:

<br/>

|                |Urbano    |Rural   | Total |
|:--------------|----------:|-------:|------:|
|Calificación   |           |        |       |
|Bueno          |20         |11      |31     |
|Regular        |40         | 8      |18     |
|Malo           |15         | 6      |21     |
|Total          |75         |25      |100    |   

<br/><br/>


**Solución**:

|                                                                                |
|:-------------------------------------------------------------------------------|
| $Ho$ : Las variables lugar de residencia y calificación son independientes     |
| $Ha$ : Las variables lugar de residencia y calificación no son independientes  |
|                                                                                |

El siguiente código da solución a la constrastación de las hipótesis

```{r}
m=c(20,40,15,11,8,6)
m=as.table(matrix(m,nrow=3))
rownames(m)=c("Bueno", "Regular", "Malo")
colnames(m)=c("Urbano", "Rural")

m

chisq.test(m)
```
Como p-value = 0.1543 lo podemos considerar grande, no se rechaza $Ho$, asumimos que $Ho$ es verdad, es decir que las variables lugar de residencia y calificación son independientes. Esto implica que la calificación dada por las personas entrevistadas no está relacionada con su calificación.





<br/><br/><br/>

### **Prueba de Wilcoxon**

|            |                                                                 |
|:---------- |:----------------------------------------------------------------|
No. de grupos|	2                                                              |
Variable dependiente| En escala al menos ordinal                               |	
Objetivo	          | Determinar su la diferencia entre la magnitud de las diferencias positivas entre los valores de las dos variables y la magnitud de las diferencias negativas es estadística mente significativas. Versión no parametrica de la prueba t para muestras pareadas                   |

<br/><br/>

**Ejemplo**

Los siguiente datos se tomaron de un estudio de comparación de adolescentes sanos (G1) y de adolescentes con bulimia (G2). El primer grupo está conformado por 15 estudiantes y el segundo grupo por 14 estudiantes y corresponden al consumo diario en calorías. Los datos obtenidos son los siguientes:  

Nota: antes de realizar la comparación de medias, se debe realizar una prueba de bondad de ajuste para establecer si los datos siguen una distribución normal o no. En caso de que los datos no tengan distribución normal, entonces se debe realizar una prueba no paramétrica.


|                                 |
|:--------------------------------|
|$Ho$ : $Me_{g1} \leq   Me_{g2}$  |
|$Ha$ : $Me_{g1} >  Me_{g2}$      |


```{r}
g1=c(68,32,58,16,23,53,55,32,61,29,50,64,67,37)
g2=c(39,10,21,29,11,26,7,12,28,32,30,27,50,19,24)
# independent 2-group Mann-Whitney U Test 
wilcox.test(g1,g2,paired = FALSE,alternative = "greater")
```

**Conclusión** :  

Se rechaza Ho , se acepta Ha. Se puede concluir que las dos poblaciones son diferentes. por lo tanto las medias son diferentes.

Para analizar cual de los grupos presenta un mayor consumo de calorías, se observa  que el Grupo I   tiene suma de rangos de 168  y el Grupo II tiene suma de rangos de 273, lo que indica que los datos del Grupo I tienen valores menores en que los del Grupo II

Por lo tanto los estudiantes sanos presentan consumo de calorías menores que los estudiantes que padecen de bulimia.

<br/><br/>

### **Prueba de Kruskal-Wallis**

|            |                                                                 |
|:---------- |:----------------------------------------------------------------|
Numero de grupos |		k                                                        |
Variable dependiente |	En escala al menos ordinal	                           |
Objetivo	            |Determinar si las diferencias entre las medias de los rangos (asignados a las observaciones ordenadas) en los k grupos son estadísticamente significativas. Versión no parametrica del ANOVA. 

<br/><br/>


**Ejemplo**

Para verificar si la memoria cambia con la edad, una investigadora realiza un estudio en el cual hay tres grupos de personas: G1- 60 años de edad, G2: con 50 años de edad, G3: con 40 años de edad. A cada persona se le presenta una serie de palabras. La serie es presentada dos veces y se cuenta el número de palabras que pueden recordar. 

|         |    |    |    |    |    |    |    |
|:-------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|Grupo 1  |28  |19  |13  |28  |29  |22  |21  | 
|Grupo 2  |26  |20  |11  |14  |22  |21  |    |
|Grupo 3  |37  |28  |26  |35  |31  |    |    |  


|                            |
|:------------------------------------------|
|$Ho$ : Las poblaciones son idénticas       |
|$Ha$ : Las poblaciones no son idénticas    |

Las anteriores hipótesis implica que sus distribuciones son las mismas (medias y varianzas iguales)


```{r}
g1=c(28,19,13,28,29,21)
g2=c(26,20,11,14,22,21)
g3=c(37,28,26,35,31)

kruskal.test(g1,g2,g3)
```

<br/><br/>

### **Correlación de Spearman**

**Ejemplo**

Se requiere establecer si existe relación entre el tiempo de estudio dedicado a la preparación de una evaluación y la nota obtenida. Como la nota obtenida representa una variable cualitativa, se debe utilizar una prueba no paramétrica (coeficiente de correlación de Spearman). Los datos obtenidos son los siguientes: 

|       |      |      |      |       |      |      |      |      |      |      |
|:------|-----:|-----:|-----:|------:|-----:|-----:|-----:|-----:|-----:|-----:|
|Tiempo | 21   |18    |15    |17     |18    |25    |18    |4     |6     | 5    |
|Nota   | 4    |4     |4     |3      |3     |5     |3     |1     | 1    | 2    |  |


```{r, message=FALSE, warning=FALSE}
x=c(21,18,15,17,18,25,18,4,6,5)
y=c(4,4,4,3,3,5,3,1,1,2)
plot(x,y, las=1, pch=19, col=c4)

cor.test(x,y, method = "spearman",continuity = FALSE,conf.level = 0.95)
```
En este caso se utiliza el coeficiente de correlación de Sperman para valorar la relación entre las variables debido a que la nota es una variable que podríamos llamar cualitativa, operacionalizada cuntitativa con una nota.


<br/><br/>

### **Prueba de Rachas**

**Ejemplo**

Para verificar si los números aleatorios generados por Excel son de verdad aleatorios se tomó una muestra de ellos mediante la función =aleatorio.entre(), obteniendo como resultado los siguientes números: 0.738, 0.202, 0.357, 0.561, 0.509, 0.146, 0.746, 0.666, 0.133, 0.430, 0.972, 0.999, 0.499, 0.869, 0.821, 0.732, 0.355, 0.189, 0.478, 0.162 .

|                            |
|:---------------------------|
|$Ho$ : La muestra es aleatoria    |
|$Ha$ : La muestra no es aleatoria |


```{r}
library("randtests")
y=rnorm(100,120,20)
runs.test(y)
```

**Conclusión**

Como el valor-p (0.3149) es mayor al nivel de significancia, no se rechaza la hipótesis nula, no existe suficiente evidencia en la muestra que permita rechazarla, asumimos que los datos son aleatorios.



<br/><br/>

### **Prueba de normalidad**

Uno de los procedimientos mas utilizados en estadística es la relacionada con la verificación de normalidad de una variable. 

|                                      |
|:-------------------------------------|
| $Ho$: X tiene distribución Normal    |
| $Ha$: X no tiene distribución Normal |

Existen muchas pruebas que nos ayudan en este propósito, a continuación relacionamos algunas de ellas 
```{r}
# se genera una variable aleatoria normal
x=rnorm(200,1000,50)
plot(density(x), main=" ", las=1, xlab = " ", ylab=" ", col=c3, lwd=4)
```

```{r}
shapiro.test(x)
```

Esta prueba no requiere la instalación de paquetes adicionales, pues está disponible en la configuración básica de R

Como el valor-p es superior al nivel de significancia $\alpha$, no se rechaza $Ho$, se asume que la distribución de la variable es normal.

<br/><br/>

### **Otras pruebas de normalidad utilizadas**


```{r}
# install.packages("normtets")
library(normtest)

jb.norm.test(x)	 # Test de normalidad de Jarque-Bera
kurtosis.norm.test(x)
skewness.norm.test(x)	
```


```{r}
# install.packages("nortets")
library(nortest)

lillie.test(x) # Kolmogorov-Smirnov
pearson.test(x) # chi-cuadrado de Pearson
```


<br/><br/>


<br/><br/><br/><br/>

## **Resumen**

<br/><br/>

|    |                 |**Estadístico de prueba**                                       |
|:--:|:----------------|:---------------------------------------------------------------|
|    |Una población    |                                                                |
|(1) |                 | $Z_{o} =\dfrac{\bar{X}-\mu_{o}}{\sigma/\sqrt{n}}\sim N(0,1)$   |
|(2) |                 | $Z_{o} =\dfrac{\bar{X}-\mu_{o}}{s/\sqrt{n}} \sim N(0,1)$       |
|(3) |                 | $T_{o} = \dfrac{\bar{X}-\mu_{o}}{s/\sqrt{n}} \sim t_{v=n-1}$   |
|(4) |                 | $X^{2}_{o} = \dfrac{(n-1)S^{2}}{\sigma_{o}^{2}} \sim \chi^{2}_{v=n-1}$ | 
|(5) |                 | $Z_{o} =  \dfrac{X-n \hspace{.1cm} p_{o}}{\sqrt{n \hspace{.1cm }p_{o}}} \sim N(0,1)$ |
|    |                 | $Z_{o} = \dfrac{\widehat{p}-p_{0}}{\sqrt{p_{o}(1-p_{o})/n}} \sim N(0,1)$ |
|    | Dos poblaciones |                                                                 |
|(6) |                 | $T_{o} =  \dfrac{\bar{d}-\Delta_{o}}{s_{d}^{2}} \sim t_{v=n-1}$ |
|    |                 | donde: $\bar{d}=\dfrac{1}{n}\displaystyle\sum_{i=1}^{n}d_{i} \hspace{.2cm}\text{ y } \hspace{.2cm} d_{i} = x_{1}-x_{2}$ |
|(7) |                 | $Z_{o} = \dfrac{(\bar{x}_{1}-\bar{x}_{2})- \Delta_{o}}{\sqrt{\dfrac{\sigma_{1}^{2}}{n_{1}}+\dfrac{\sigma_{2}^{2}}{n_{2}}}} \sim N(0,1)$|
|(8) |                 | $T_{o} = \dfrac{(\bar{x}_{1}-\bar{x}_{2})-\Delta_{o}}{s_{p}\sqrt{\dfrac{1}{n_{1}} + \dfrac{1}{n_{2}}}} \sim t_{v=n_{1}+n_{2}-2}$|
|    |                 | donde: $s^{2}_{p}=\dfrac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s^{2}_{2}}{n_{1}+n_{2}-2}$ |
|(9) |                 | $T_{o} =  \dfrac{(\bar{x}_{1}-\bar{x}_{2}-\Delta_{o}}{\sqrt{\dfrac{s^{2}_{1}}{n_{1}}+\dfrac{s^{2}_{2}}{n_{2}}}} \sim t_{v^{*}}$ |
|    |                 | donde: $v^{*}= \dfrac{(s^{2}_{1}/n_{1} + s^{2}_{2}/n_{2})^{2}}{\dfrac{(s^{2}_{1}/n_{1})^{2}}{n_{1}-1}+\dfrac{(s^{2}_{2}/n_{2})^{2}}{n_{2}-1}}$|
|(10)|                 | $Z_{o} =  \dfrac{(\widehat{p_{1}}-\widehat{p_{2}})-\Delta_{o}}{\sqrt{\widehat{p}\widehat{q}(1/n_{1}+1/n_{2})}}$ |
|    |                 | donde: $\widehat{p}=\dfrac{x_{1}+x_{2}}{n_{1}+n_{2}}$ \hspace{.2cm} $\widehat{q}=1-\widehat{p}$, |
|    |                 | $\widehat{p_{1}}=\dfrac{x_{1}}{n_{1}} \hspace{.1cm} \text{ y } \hspace{.1cm}\widehat{p_{2}}=\dfrac{x_{2}}{n_{2}}$ |
|(11)|                 | $F =  \dfrac{s_{1}^{2}}{s_{2}^{2}} \sim F_{v_{1}:n_{1}-1; v_{2}: n_{2}-1}$ |
|    |                 | |

<br/><br/>

|                 |  **Reglas de decisión**                                        |
|:----------------|:---------------------------------------------------------------|
| Regla1:         | Si el $EdeP$ cae en la $RdeR$, entonce se rechaza la $H_{0}$ y se **acepta** $H_{a}$ como verdadera. Si por el contrario el $EdeP$ NO cae en la $RdeR$, entonces NO se rechaza $H_{0}$, no existe suficiente evidencia para rechazarla, **asumimos** que $H_{0}$ es verdad.|
| Regla2:         | Si $\alpha >$ `valor-p` entonces rechazamos $H_{0}$, se **acepta** $H_{a}$. Si por el contrario $\alpha<$ `valor-p`, no rechazamos $H_{0}$, **asumimos** 	que $H_{0}$ es verdad. |
| Regla3:         | El `valor-p` se interpreta como el error que puedo cometer al rechazar $H_{0}$, siendo esta verdadera (cometer error tipo I). Si este valor es considerado como pequeño, rechazo $H_{0}$, se **acepta** $H_{a}$. Si por el contrario se considera este valor grande, entonce no rechazo $H_{0}$, asumo que $H_{0}$ es verdad.|


```{r}

```

<br/><br/>

```{r, echo=FALSE, out.width="80%", fig.align = "center"}
#knitr::include_graphics("img/relacionPdeH.png")
```


<br/><br/>

## **Código R**

```{r, echo=FALSE, out.width="10%"}
#knitr::include_graphics("img/logoR.png")
```

|                   |                                                          |
|:------------------|:---------------------------------------------------------|
|                   |**Pruebas paramétricas**                                  |
|Entrada de datos   |                                                          |
|                   |`x1=c(7, 13, 6, 5, 5, 10, 8, 6, 7)`                       |
|                   |`x2=c(3,7,2,3,6,2,1,0,2)`                                 |  
|Una población      |                                                          |
||`z.test(datos,mu=10,stdev=4, conf.level=0.98)`                               |
||`t.test(datos, mu=10,conf.level=0.98)`                                       |
||`t.test(datos, mu=10,conf.level=0.98,alternative="greater")`                 |
||`t.test(datos, mu=10,conf.level=0.98,alternative="less)`                     |
||`prop.test(x=22,n=100, p=0.20, conf.level=0.98)`                             |
|Dos poblaciones    |                                                          |
|                   |`t.test(x1,x2,  paired=TRUE)`                             |    
|                   |`t.test(x1,x2,  paired=FALSE, var.equal=TRUE, conf.level=0.95)` |
|                   |`t.test(x1,x2,  paired=FALSE, var.equal=FALSE, conf.level=0.98)`|
|                   |`var.test(x,y)`                                           |
|                   |`prop.test(c(x1,x2), c(n1,n2))`                           |
|                   |**Pruebas no paramétricas**                               |
|                   | `chisq.test(x=obs,p=esp)`                                |
| paquete: MASS     | `library(MASS)`                                          |
|                   |`chisq.test(M)`  # M: matriz                              |
|paquete: BSDA      |`library(BSDA)`                                           |
|                   |`SIGN.test(x,md=15,alternative = "greater")`              |
|                   |`wilcox.test(g1,g2,paired = FALSE,alternative = "less")`  |
|                   |**Pruebas de normalidad**                                 |
|                   |`shapiro.test(x)`                                         |
| paquete:normtest  |` library(normtest)`                                      |
|                   |`ajb.norm.test(x)`                                        |
|                   |`frosini.norm.test(x)`                                    |
|                   |`geary.norm.test(x)`                                      |
|                   |`hegazy1.norm.test(x)	`                                  |
|                   |`hegazy2.norm.test(x)`                                    |
|                   |`jb.norm.test(x)	`                                      |
|                   |`kurtosis.norm.test(x)`                                   |
|                   |`skewness.norm.test(x)	`                                  |
|                   |`spiegelhalter.norm.test(x)`                              |
|                   |`wb.norm.test(x)	`                                      |
|paquete: nortest   |`library(nortest)`                                        |
|                   |`ad.test(x)`                                              |
|                   |`cvm.test(x)`                                             |
|                   |`lillie.test(x)`                                          |
|                   |`pearson.test(x)`                                         |
|                   |`sf.test(x)`                                              |

<br/><br/>


|Tipos de pruebas   |                                                                 |
|:------------------|:----------------------------------------------------------------|
|Dos colas          |$H_{0}: \theta = \theta_{o}$ vs $H_{0}: \theta \neq \theta_{o}$  |
|Cola superior      |$H_{0}: \theta \leq \theta_{o}$ vs $H_{0}: \theta > \theta_{o}$  |
|Cola inferior      |$H_{0}: \theta \geq \theta_{o}$ vs $H_{0}: \theta < \theta_{o}$  |

<br/><br/>
